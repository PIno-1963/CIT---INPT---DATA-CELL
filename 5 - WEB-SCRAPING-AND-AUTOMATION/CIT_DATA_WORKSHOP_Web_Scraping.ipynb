{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djImSTy0XAs4"
      },
      "source": [
        "<center> <img src=https://github.com/PIno-1963/CIT---INPT---DATA-CELL/blob/main/1%20-%20CIT-DATA-101/CIT-DATA-HEADER.png?raw=true> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZQ35znBBGHm"
      },
      "source": [
        "<center><h1 >Scraping Data</h1></center>\n",
        "\n",
        "Welcome to this notebook, where we will explore the exciting journey of web scraping images from [Unsplash](Unsplash.com) and utilizing them to train a Deep Learning (DL) model. Unsplash is a vast repository of high-quality images, making it an ideal source for diverse datasets.\n",
        "\n",
        "In this notebook, our primary objectives are:\n",
        "\n",
        "**Data Scraping:**\n",
        "We will leverage web scraping techniques to gather images from Unsplash. Specifically, we'll focus on scraping images from two distinct classes to build a diverse dataset.\n",
        "\n",
        "**Data Preprocessing:**\n",
        "After obtaining the images, we'll perform essential preprocessing tasks such as resizing, normalizing, and organizing the data to prepare it for model training.\n",
        "\n",
        "**Deep Learning Model:**\n",
        "We'll design and train a DL model using a popular framework TensorFlow. The model will be tailored to classify images belonging to the two classes we scraped.\n",
        "\n",
        "**Model Evaluation:**\n",
        "Once the model is trained, we'll assess its performance using evaluation metrics and visualize its predictions on unseen data.\n",
        "By the end of this notebook, you'll have gained hands-on experience in web scraping, data preprocessing, and building and training a DL model for image classification. Let's dive in and unlock the potential of extracting knowledge from the vast visual resources available on Unsplash!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qTCxj8WEyYa"
      },
      "outputs": [],
      "source": [
        "!pip install pathlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsVe0HTauZvP"
      },
      "source": [
        "### Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJZ3S8NGKF2h"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTdG-bAjt013"
      },
      "source": [
        "### **TASK 1** :\n",
        "\n",
        "Create a directory named \"images,\" and within it, add two subdirectories named as the chosen classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8xboQ55C3bF"
      },
      "outputs": [],
      "source": [
        "# Use ! to write bash commands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZWMIuYtufOL"
      },
      "source": [
        "### Creating Scraping functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy7K0NQlPKRy"
      },
      "source": [
        "### **TASK 2**:\n",
        "\n",
        "Write a Python function that downloads content and saves it as an image file. The function should take three parameters:\n",
        "\n",
        "- data: The binary content of the image.\n",
        "- category: The name of the class or category for organizing the images.\n",
        "- filename: The name of the image file.\n",
        "\n",
        "The function should save the image file in the `\"/content/images/class_name/image_name.jpeg\"` directory, within a subdirectory named after the specified category.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9pQi6SiSR41"
      },
      "outputs": [],
      "source": [
        "def download_image(image_content, class_name, file_name):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEmvzL8yPZ8L"
      },
      "source": [
        "### **TASK 3**\n",
        "\n",
        "Write a function that send a **GET** request to an URL and return the response content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9L9wF7sW3Yl"
      },
      "outputs": [],
      "source": [
        "def get_data(url):\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkx-hfkaYTGf"
      },
      "source": [
        "### **TASK 4**\n",
        "\n",
        "Create a function that accepts a JSON response from Unsplash and extracts all image links contained within that response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTlf01LiScx3"
      },
      "outputs": [],
      "source": [
        "def get_images_links(response):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DyMwIjUPdi2"
      },
      "source": [
        "### **TASK 5**\n",
        "\n",
        "Create a function that orchestrates the entire process, covering the following steps:\n",
        "\n",
        "1. **Getting Data from the Unsplash API:**\n",
        "   Fetch data from the Unsplash API to acquire information about images.\n",
        "\n",
        "2. **Extracting Image Links from the Response:**\n",
        "  Parse the JSON response from Unsplash to extract links to the images.\n",
        "\n",
        "3. **Downloading Each Image to the Correct Path:**\n",
        "   Iterate through the extracted image links and download each image, placing them in the appropriate path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VEoUezWBJxH"
      },
      "outputs": [],
      "source": [
        "def scrap_images(class_name):\n",
        "    # Initialize the counter for the number of scraped images\n",
        "    num_scraped_images = 0\n",
        "\n",
        "    # Iterate through 50 pages (adjust as needed)\n",
        "    for i in tqdm(range(50)):\n",
        "        # Step 1: Getting Data from the Unsplash API\n",
        "        # Make a request to the Unsplash API to get information about images\n",
        "        response = get_data(\n",
        "            f\"\"\"https://unsplash.com/napi/search/photos?query={class_name}&\n",
        "                                per_page=30&page={i}&xp=search-synonym%3Acontrol\"\"\"\n",
        "        )\n",
        "\n",
        "        # Parse the JSON response\n",
        "        response_json = json.loads(response)\n",
        "\n",
        "        # Step 2: Extracting Image Links from the Response\n",
        "        # Extract links to the images from the JSON response\n",
        "        images_links = ____\n",
        "\n",
        "        # Step 3: Downloading Each Image to the Correct Path\n",
        "        # Iterate through the extracted image links\n",
        "        for link in images_links:\n",
        "            try:\n",
        "                # Get image data from the link (GET request)\n",
        "                image = ____\n",
        "\n",
        "                # Download the image and save it to the appropriate path\n",
        "                download_image(____, ____, f'{num_scraped_images:03}')\n",
        "\n",
        "                # Increment the counter for scraped images\n",
        "                num_scraped_images += 1\n",
        "\n",
        "            except:\n",
        "                # If an exception occurs (e.g., image download fails), continue to the next image\n",
        "                continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3ZkHoMTrT69"
      },
      "source": [
        "Let's proceed to download the images for the selected classes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">  Change \"category_1\" and \"category_2\" with the two image categories of your choice"
      ],
      "metadata": {
        "id": "-Fo6uefIL1-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y3rDgUeBkg6"
      },
      "outputs": [],
      "source": [
        "# Make sure to change \"category_1\" and \"category_2\"\n",
        "for cls in [\"category_1\", \"category_2\"]:\n",
        "    scrap_images(cls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blAFBQIL-Ck1"
      },
      "source": [
        "<center><h1>Working with CNN</h1></center>\n",
        "\n",
        "In this section, we will utilize the scraped data to train a model for image classification.\n",
        "\n",
        "First, we should create a DataFrame that organizes image paths and labels based on the directory structure, making it easier to work with the data for tasks such as image classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLLvjFb69_qa"
      },
      "outputs": [],
      "source": [
        "path = Path(\"/content/images/\")\n",
        "\n",
        "db = {\"image\": [] , \"label\": []}\n",
        "\n",
        "classes = os.listdir(path)\n",
        "\n",
        "for cls in classes:\n",
        "\n",
        "  images = os.listdir(path / cls)\n",
        "  db[\"image\"].extend([*map(lambda x : str(path/ cls / x), images)])\n",
        "  db[\"label\"].extend([cls]*len(images))\n",
        "\n",
        "db = pd.DataFrame(db)\n",
        "db.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPuc-rxEv41H"
      },
      "source": [
        "### **TASK 6**\n",
        "\n",
        "Visualize the distribution of images across each class by plotting a bar chart. Generate a similar graph using `matplotlib` or `seaborn`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig=px.bar(db.groupby(\"label\").count())\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "el2Ug-qMMHvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvTVI6DRdDq6"
      },
      "outputs": [],
      "source": [
        "# Write your data visualization code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzkVcFi-wkS8"
      },
      "source": [
        "Split the DataFrame into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5n-QeJjBD34"
      },
      "outputs": [],
      "source": [
        "# Split your data into training and testing sets (30% test size)\n",
        "X_train , X_test = train_test_split(db,test_size=.3,random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzVSBdmexiRW"
      },
      "source": [
        "Display some images from training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fDvDHx_BIpZD"
      },
      "outputs": [],
      "source": [
        "samples = X_train.sample(9)\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3, i + 1)\n",
        "  plt.imshow(cv2.cvtColor(cv2.imread(samples.iloc[i,0]), cv2.COLOR_RGB2BGR))\n",
        "  plt.title(samples.iloc[i,1])\n",
        "  plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HNR7hqu0tz6"
      },
      "source": [
        "This code is using the `ImageDataGenerator` from TensorFlow's **Keras** API to generate batches of image data for training and testing. It is specifically designed to work with data stored in a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I-hCzZ19GT5O"
      },
      "outputs": [],
      "source": [
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "\n",
        ")\n",
        "\n",
        "x_train = img_gen.flow_from_dataframe(dataframe=X_train,\n",
        "                                      x_col=\"image\",\n",
        "                                      y_col=\"label\",\n",
        "                                      shuffle = False,\n",
        "                                      target_size=(224,224))\n",
        "\n",
        "x_test = img_gen.flow_from_dataframe(dataframe=X_test,\n",
        "                                     x_col=\"image\",\n",
        "                                     y_col=\"label\",\n",
        "                                     shuffle= False,\n",
        "                                     target_size=(224,224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hksnFFi50_93"
      },
      "source": [
        "Let's proceed with building our model. Given that we might not have sufficient data to train a model from scratch, we will employ a 'weight extraction' approach using EfficientNetB0. This involves utilizing the pre-trained weights from EfficientNetB0 as a starting point for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PlopTIUCI-nS"
      },
      "outputs": [],
      "source": [
        "effnet = tf.keras.applications.EfficientNetB0(weights='imagenet',\n",
        "                                              include_top=False,input_shape=(224,224,3))\n",
        "\n",
        "effnet.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aEyGHPWrJG3z"
      },
      "outputs": [],
      "source": [
        "input = tf.keras.layers.Input((224,224,3))\n",
        "x = effnet(input)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(rate=0.4)(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "model = tf.keras.models.Model(inputs = input , outputs = output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaGcFX8u4GJk"
      },
      "source": [
        "Let's compile our model using the $\\text{Categorical Crossentropy}$ loss function, the $\\text{Adam optimizer}$ with a specified learning rate, and accuracy as the metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bP2IXUe-JM6z"
      },
      "outputs": [],
      "source": [
        "model.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              metrics = [\"accuracy\"])\n",
        "\n",
        "print(\"Model compiled!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSGCgH0g5HSw"
      },
      "source": [
        "now let's train our model, but first we should ensure that training will be terminated  once the model's performance on a validation dataset stops improving, we will use an EarlyStopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kGl1MACaRNER"
      },
      "outputs": [],
      "source": [
        "earlystop = EarlyStopping(\n",
        "                  monitor=\"val_loss\",\n",
        "                  patience=5,\n",
        "                  verbose=1\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMp5mQhoRsWp"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train,\n",
        "                    validation_data=x_test,\n",
        "                    batch_size=32,\n",
        "                    epochs=5,\n",
        "                    shuffle= True,\n",
        "                    callbacks=[earlystop])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piCNYQae6NKx"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_JUIATLRvhi"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(x_test)\n",
        "\n",
        "print(\"loss:\",loss)\n",
        "print(\"Accuracy:\",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display our model's confusion matrix"
      ],
      "metadata": {
        "id": "SsX1coX2NCRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSGm7DkvBZ8S"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "classes = x_test.class_indices\n",
        "\n",
        "predictions = [*map(lambda x: np.argmax(x),model.predict(x_test ,verbose = 0))]\n",
        "\n",
        "true_labels = [*map(lambda x: classes[x] , X_test[\"label\"].values)]\n",
        "\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes.keys())\n",
        "\n",
        "disp.plot(cmap = \"Blues\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V0T3Q9VBT1R"
      },
      "source": [
        "## Testing model on data from Internet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYDOPTb9HJWC"
      },
      "outputs": [],
      "source": [
        "classes = list(x_train.class_indices.keys())\n",
        "\n",
        "def predict(image_url):\n",
        "    global classes\n",
        "\n",
        "    img = cv2.imread(image_name)\n",
        "    img = cv2.resize(img, (224,224))\n",
        "\n",
        "    prediction = model.predict(img.reshape(1,224,224,3),\n",
        "                               verbose = 0)\n",
        "\n",
        "    pred_class = classes[np.argmax(prediction)]\n",
        "    prob = np.max(prediction) * 100\n",
        "\n",
        "    # show the image and the prediction\n",
        "    plt.imshow(cv2.cvtColor(img , cv2.COLOR_BGR2RGB))\n",
        "    plt.title(pred_class + \" \" + str(prob) + \"%\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCPmpKowKz3X"
      },
      "outputs": [],
      "source": [
        "!wget -O #your_filename:________  #image_URL: _______"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sipGdmNHM4Dr"
      },
      "outputs": [],
      "source": [
        "# Rewrite the filename down below to predict your image class\n",
        "predict(\"______\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}